{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d7f53f-32c8-40ba-982d-04c7548932b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a76be2-1041-4fc6-b94b-ab46b78e169c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9092a1b-d2bd-4aa0-ad8f-be5f1a47da77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage1](stage1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9435574f-1e19-4ddd-acc4-b5f11c5fcd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Passive Data Munging** - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "\n",
    "**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc009f1a-5864-4b7b-8697-1dd72790eb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging - \n",
    "- Visible - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76842c2-34fc-43a7-8138-e331de2af1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns and null records are there\n",
    "- - duplicate rows & Duplicate id keys\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ccadc2-69ed-4744-8673-b83a29cd0eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b7e978-9f46-4506-8ab2-c79f8bcf1089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870e142d-523c-49a3-8c70-1cbd96d00377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "rawdf1.printSchema()#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0])\n",
    "\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9ed1b9-96c1-4f58-939a-320911e29b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccb7e0e-aeec-4789-aecd-61187cacd34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429a7a56-fe79-4efa-b0d4-74193d3705e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession#15lakhs\n",
    "spark=SparkSession.builder.appName(\"WD36 - ETL Pipeline - Bread & Butter\").getOrCreate()#3 lakhs LOC by Databricks (for eg. display, delta, xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c30b1f-5451-4ca3-855e-cb218b514988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. **Structuring** - Combining Data + Schema Evolution/Merging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5523010-79be-4bf2-9230-1f6eb813710c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When do we go for Schema Evolution?<br>\n",
    "Over the time, if no. of col are keep added by source<br>\n",
    "Serialization  while writing+ mergeSchema while reading<br>\n",
    "When do we go for Schema Merging?<br>\n",
    "In a given day, If we get multiple files of related (not same) structure<br>\n",
    "After reading in dataframe format -> unionByName + allowMissingColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbd0c39-1220-45a5-96dd-8ac93d4990cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/custsmodified\")\n",
    "#2. Multiple files (with same or different names)\n",
    "rawdf1_1=spark.read.schema(struct1).csv(path=[\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/custsmodified\",\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/custsmodified\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1_2=spark.read.schema(struct1).csv(path=[\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/\",\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d3ddef-c256-474f-b8de-cefad4bdb5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Active Data munging...\n",
    "#When you go for Schema Merging/Melting and Schema Evolution?\n",
    "#Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "#Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "#when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution\n",
    "\n",
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf1)\n",
    "display(rawdf2)\n",
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype..\n",
    "display(rawdf_merged)\n",
    "#Expected right approach to follow\n",
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)\n",
    "\n",
    "#Here, we are merging two files because both are in CSV format. If one file is CSV and the other file is in a different format, what should we do in this scenario? it will be handled automatically\n",
    "#rawdf2.write.json(\"/Volumes/workspace/wd36schema/ingestion_volume/staging/csvjson\")\n",
    "#rawdf3=spark.read.json(\"/Volumes/workspace/wd36schema/ingestion_volume/staging/csvjson\")\n",
    "#rawdf_merged=rawdf_merged.unionByName(rawdf3,allowMissingColumns=True)\n",
    "#display(rawdf_merged)#Expected dataframe to proceed further munging on a single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee2be30-7760-4f90-b0e3-90be3b38a04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Just for the simple learning of schema evolution & schema merging/melting<br>\n",
    "Schema merging/melting<br>\n",
    "1,rajeshwari -day1(source1)<br>\n",
    "1,rajeshwari,30 d-ay1(source2)<br>\n",
    "\n",
    "Schema evolution<br>\n",
    "1,rajeshwari day1<br>\n",
    "1,rajeshwari,30 day2<br>\n",
    "\n",
    "Output is same in both cases...<br>\n",
    "id,name,age<br>\n",
    "1,rajeshwari,null<br>\n",
    "1,rajeshwari,30<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e04175e7-a081-4ed6-873c-95e6d32d67e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353ab49f-a337-45fd-9828-20626605eed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "#print(rawdf1.schema)\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/custsmodified\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\",cleandf1.count())#all rows count\n",
    "display(cleandf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/custsmodified\",mode='dropMalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleandf1.collect()))\n",
    "display(cleandf1)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)\n",
    "print(cleandf1.count())#count will return the original count of the raw data\n",
    "print(len(cleandf1.collect()))#collect+len will return the dropmalformed count of the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4c6278-9af0-413b-8265-f7fb2ac26590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be80d61-3a5e-48d5-9469-3bad93e34dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "struttype1=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf1=spark.read.schema(struttype1).csv(path=\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/custsmodified\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf1.count())#all rows count\n",
    "display(rawdf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a6f1dd-3f3a-41e0-a71e-256470c0d6cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dca0c0d-a8b0-4290-969c-0aed494b18f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating rejection dataset to send to our source system for future fix\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/custsmodified\",mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\")\n",
    "#Create a reject dataset\n",
    "rejectdf1=cleandf1.where(\"corruptedrows is not null\")\n",
    "#display(rejectdf1)\n",
    "rejectdf1.write.csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/reject\",mode=\"overwrite\",header=True)\n",
    "retaineddf1=cleandf1.where(\"corruptedrows is null\")\n",
    "print(\"Overall rows in the source data is \",len(cleandf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(rejectdf1.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retaineddf1.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf841cb-a560-4535-8fe1-480a11915ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing \n",
    "na.drop()<br>\n",
    "It is a process of cleaning/removing/deleting unwanted data\n",
    "Eg. I am purchasing potato from a shop, I am cutting down the debris/rotten portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105ae6b5-59ee-46a2-8459-410cff2392e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\")#This function will drop any column in a given row with null otherwise this function returns rows with no null columns - In a scenario of if the source send the Datascience Model features (we shouldn't have any one feature with null value, hence we can use this function)\n",
    "print(\"any one row in the raw df with age null\")\n",
    "display(rawdf1.where(\"age is null\"))\n",
    "print(\"any one row in the cleansed df with age null\")\n",
    "display(cleanseddf.where(\"age is null\"))#any one column contains null will be cleaned\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\",subset=[\"id\",\"age\"])#If we need CDE without nulls (Critical Data Elements/Significant columns) columns\n",
    "print(\"any one row in the cleansed df with id or age null\")\n",
    "display(cleanseddf)\n",
    "cleanseddf=rawdf1.na.drop(how=\"all\",subset=[\"firstname\",\"lastname\"])#4000004,Gretchen,,66,\n",
    "print(\"any one row in the cleansed df with firstname and lastname is null\")\n",
    "print(\"Total rows without firstname and lastname with null values\",len(cleanseddf.collect()))\n",
    "display(cleanseddf)#We are taking this DF further for munging.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44cc3ee-eaab-4159-b2ac-2327336e69ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing \n",
    "na.fill() & na.replace()<br>\n",
    "It is a process of polishing/fine tuning/scrubbing/meaningful conversion the data in a usable format\n",
    "Eg. I am purchasing potato from a shop, I am scrubbing/washing mud/sand portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3d8888-1919-4c79-9109-001b78e32e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbeddf1=cleanseddf.na.fill('not provided',subset=[\"lastname\",\"profession\"])#fill will help us replace nulls with some value\n",
    "display(scrubbeddf1)\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "scrubbeddf2=scrubbeddf1.na.replace(find_replace_values_dict1,subset=[\"profession\"])#fill function is helping us find and replace the values\n",
    "scrubbeddf3=scrubbeddf2.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "display(scrubbeddf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408ab283-a274-4153-b70a-ba12892ae32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication\n",
    "Removal of duplicate rows/columns based on a priority or non priority\n",
    "distinct & dropDuplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4102f20-c791-4744-ad4c-80ed274e9ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scrubbeddf3.where(\"id in ('4000001')\"))#before row level dedup\n",
    "dedupdf1=scrubbeddf3.distinct()#It will remove the row level duplicates\n",
    "display(dedupdf1.where(\"id in ('4000001')\"))\n",
    "\n",
    "print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))\n",
    "print(\"prioritized deduplication based on age\")\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "#dedupdf1.coalesce(1).where(\"id in ('4000003')\").orderBy([\"id\",\"age\"],ascending=[True,False]).show(3)\n",
    "dedupdf2=dedupdf1.coalesce(1).orderBy([\"id\",\"age\"],ascending=[True,False]).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28e5dec-faa2-4496-9f39-125fb311037f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39eab153-273f-42af-ba1d-e54a1a20308b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Standardization - \n",
    "Making the data more standard by adding/removing/reordering columns as per the expected standard, unifying into expected format, converting the type as expected etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4c2d45-2a72-4b38-9500-01246274b241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization1 - Column Enrichment (Addition of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29dabdf8-f572-46c5-9560-e77bfc698d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap,col\n",
    "#withColumn(\"stringcolumnname to add in the df\",lit('hardcoded')/initcap(col(\"colname\")))\n",
    "standarddf1=dedupdf2.withColumn(\"sourcesystem\",lit(\"Retail\"))#SparkSQL - DSL(FBP)\n",
    "display(standarddf1.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725720fa-2cbc-4754-8009-c0d4ab855390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52d29ee-e00e-4460-b258-5647e8dc10c7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767060087379}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "#Basic Exploration/analysis of the profession column for identifying uniformity challenges\n",
    "#standarddf1.createOrReplaceTempView(\"sqlview\")\n",
    "#display(spark.sql(\"select profession,count(*) from sqlview group by profession order by profession\"))#SQL\n",
    "#display(standarddf1.groupBy(\"profession\").count())#DSL\n",
    "#Standardization2 - column uniformity\n",
    "standarddf2=standarddf1.withColumn(\"profession\",initcap(\"profession\"))#inicap or any other string function with columnOr name can accept either column or string type provided if the string is a column name for eg. profession/age/sourcesystem.\n",
    "display(standarddf2.limit(20))\n",
    "#display(standarddf2.groupBy(\"profession\").count())#DSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd29200-6a86-45b8-b4ff-9d450113a93c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d057244-324b-44c1-8df6-1ca65aad0245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Did analysis to understand the format issues in our id and age columns\n",
    "#standarddf2.where(\"id like 't%'\").show()\n",
    "standarddf2.where(\"id rlike '[a-zA-Z]'\").show()#rlike is regular expression like function that help us identify any string data in our DF column\n",
    "standarddf2.where(\"age rlike '[^0-9]'\").show()#checking for any non number values in age column\n",
    "standarddf3=standarddf2.withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee246c0c-04ad-4b56-b747-9b95f272c124",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767063664438}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,replace\n",
    "#Let's apply scrubbing features to our id column to replace ten with 10 (or we can think of using GenAI here)\n",
    "replaceval={'one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9','ten':'10'}\n",
    "standarddf3=standarddf2.na.replace(replaceval,[\"id\"])\n",
    "#standarddf3=standarddf2.withColumn(\"id\",replace(\"id\",lit('ten'),\"10\"))\n",
    "standarddf3=standarddf3.withColumn(\"age\",regexp_replace(\"age\",'-',\"\"))\n",
    "display(standarddf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3aa19c5-51e2-4df7-9be4-17102ac8f538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization4 - Data Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c584abc2-9715-49fc-951d-eccd85c443d1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767064855970}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf3.printSchema()#Still id and age are string type, though it contains int data\n",
    "#standarddf4=standarddf3.withColumn(\"id\",\"id\".cast(\"long\"))#this wil not work\n",
    "standarddf4=standarddf3.withColumn(\"id\",standarddf3.id.cast(\"long\"))\n",
    "standarddf4=standarddf3.withColumn(\"id\",standarddf3[\"id\"].cast(\"long\"))\n",
    "standarddf4=standarddf3.withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standarddf4=standarddf4.withColumn(\"age\",col(\"age\").cast(\"short\"))\n",
    "standarddf4.printSchema()\n",
    "display(standarddf4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b21a48f-a296-4d62-a155-254eda8ad66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca72ea8-f525-4b27-b049-4a5f47c7261c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767065410290}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf5=standarddf4.withColumnRenamed(\"id\",\"custid\")\n",
    "standarddf5=standarddf4.withColumnsRenamed({\"id\":\"custid\",\"sourcesystem\":\"srcsystem\"})\n",
    "display(standarddf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0bcf76-6828-4c29-8f12-b1431579b48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649e2e88-6375-4e18-ad01-7df70bf636de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6=standarddf5.select(\"custid\", \"age\", \"firstname\",\"lastname\",\"profession\",\"srcsystem\")\n",
    "#display(standarddf6)\n",
    "mungeddf=standarddf6\n",
    "display(mungeddf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f3fd5d-5d69-47f6-824d-5891c9e47b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting Data Enrichment or before sharing the data to the consumer, we have to do EDA/Exploration/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c874135-22fb-4d2d-8572-ec60089ff1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mungeddf.printSchema()\n",
    "display(mungeddf.take(20))\n",
    "display(\"total rows\",len(mungeddf.collect()))\n",
    "display(mungeddf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c13f8c-d526-41c4-9379-50dd4e36a31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**2. Data Enrichment** - Detailing of data\n",
    "Makes your data rich and detailed <br>\n",
    "a. Add (), Derive (), Remove/Elimi), Rename (), Modify/replace () - very important spark sql functions <br>\n",
    "b. split, merge/Concat <br>\n",
    "c. Type Casting, reformat & Schema Migration <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a098e90-94c8-4314-8b46-352308b1259a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![stage2](stage2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89a3cc4-1b77-454d-b38e-35b1cea6ef27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####a. Add (), Derive (), Rename (), Modify/replace (), Remove/Eliminate () - very important spark sql DF functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51bcd36c-3e69-4438-8130-991b51f7489f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Adding of columns\n",
    "Lets add datadt (date of the data orgniated from the source for eg. provided in the filename in a format of yy/dd/MM) and loaddt (date when we are loading the data into our system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7673e2a1-5f64-4e6c-881c-985305f6306c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_datadt='25/30/12'\n",
    "print(f\"hello '{derived_datadt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cfe5ae-a1bf-482b-9561-6ef8061556fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,current_date#already imported, not needed here\n",
    "original_filename='custsmodified_25/30/12.csv'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "#derived_datadt='25/30/12'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "enrichdf1=mungeddf.withColumn(\"datadt\",lit('25/30/12')).withColumn(\"loaddt\",current_date())\n",
    "enrichdf1.printSchema()\n",
    "#or\n",
    "enrichdf1=mungeddf.withColumns({\"datadt\":lit('25/30/12'),\"loaddt\":current_date()})\n",
    "enrichdf1.printSchema()\n",
    "#or\n",
    "enrichdf1=mungeddf.select(\"*\",lit(derived_datadt).alias('datadt'),current_date().alias('loaddt'))#DSLs (FBP function)\n",
    "#or\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",\"'25/30/12' as datadt\",\"current_date() as loaddt\")#DSL(select) + SQL expression\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",f\"'{derived_datadt}' as datadt\",\"current_date() as loaddt\")#DSL(select) + SQL expression\n",
    "enrichdf1.printSchema()\n",
    "display(enrichdf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ba0edf-3d6a-4867-96c4-73bd440b0436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f938be6a-5cc6-404b-8cfd-46a37826741c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767149978456}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "enrichdf2=enrichdf1.withColumn(\"professionflag\",substring(\"profession\",1,1))\n",
    "#or\n",
    "enrichdf2=enrichdf1.select(\"*\",substring(\"profession\",1,1).alias(\"professionflag\"))\n",
    "#or\n",
    "enrichdf2=enrichdf1.selectExpr(\"*\",\"substr(profession,1,1) as professionflag\")\n",
    "display(enrichdf2.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b022c608-ca0c-4d00-a0ea-12f229720cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Renaming of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43cf5f1-a1f5-4d6d-b802-716084b739b9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767150783310}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Can we use withColumn to rename? not directly, its costly\n",
    "enrichdf3=enrichdf2.withColumn(\"sourcename\",col(\"srcsystem\"))\n",
    "enrichdf3=enrichdf3.drop(\"srcsystem\").select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"professionflag\")\n",
    "#or\n",
    "enrichdf3=enrichdf2.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",col(\"srcsystem\").alias(\"sourcename\"),\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "#enrichdf2.printSchema()\n",
    "enrichdf3=enrichdf2.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"srcsystem as sourcename\",\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnRenamed(\"srcsystem\",\"sourcename\")#Best function to rename the column(s)\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnsRenamed({\"srcsystem\":\"sourcename\",\"professionflag\":\"profflag\"})\n",
    "display(enrichdf3.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76988923-2c67-42f7-9468-0dd2e48ec395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b7eee8-ced4-4847-9cda-a1a911df9345",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767151701508}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#This will replace the profession with sourcename\n",
    "#or\n",
    "enrichdf4=enrichdf3.withColumn(\"profession\",concat(\"profession\",lit('-'),\"profflag\"))#This will modify/enrich the profession column with sourcename\n",
    "#or using select/selectExpr\n",
    "enrichdf4=enrichdf3.select(\"custid\",\"age\",\"firstname\",\"lastname\",concat(\"profession\",lit('-'),\"profflag\").alias(\"profession\"),\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "#or use selectExpr\n",
    "enrichdf4=enrichdf3.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"concat(profession,'-',profflag) as profession\",\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "display(enrichdf4.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06206e99-f033-40c1-9a61-70533cf8a0b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Remove/Eliminate (drop,select,selectExpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ba031e-17b9-4789-9455-ec4c6074d7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#Cannot be used\n",
    "#or using select/selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or use selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or \n",
    "enrichdf5=enrichdf4.drop(\"profflag\")#right function to use from dropping\n",
    "display(enrichdf5.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50725b0d-dea8-42a3-bc83-aeb480453caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a='irfan'\n",
    "sqlexpression=f\"'{a}' as owner\"\n",
    "print(sqlexpression)\n",
    "mungeddf.selectExpr(\"*\",sqlexpression).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cf160f-2145-4f3a-addd-bc2413083ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Conclusion/Best practices of using different column enrichment functions\n",
    "1. select is good to use if we want to perform - \n",
    "Good for ordering/reordering, only renaming column (not good), only reformatting/deriving a column (not good), **for all of these operation in a single iteration** such renaming, reordering, reformatting,deriving, dropping etc., (best to use)\n",
    "2. selectExpr is good to use if we want to perform - Same as select by using iso sql functionality (if we are not familiar in DSL) **for all of these operation in a single iteration**\n",
    "3. withColumn is good to use if we want to perform - \n",
    "**for adding/deriving/modifying/replacing in a single iteration**\n",
    "Adding/Deriving column(s) in the last (Good), Modifying/replacing (Good), Renaming (not good), Dropping(not possible)\n",
    "4. withColumnRenamed is good to use if we want to perform - only for renaming column (Good)\n",
    "5. drop is good to use if we want to perform - only dropping of columns in the given position (Good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c405860d-be2c-4b48-930b-18e9c327f551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####b. Splitting & Merging/Melting of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1317c9b5-61b0-4b5f-bd5d-8575b514faf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of column\n",
    "splitdf=enrichdf5.withColumn(\"profflag\",split(\"profession\",'-'))\n",
    "splitdf=splitdf.withColumn(\"profession\",col(\"profflag\")[0])\n",
    "#splitdf=splitdf.withColumn(\"profflag\",col(\"profflag\")[1])\n",
    "#or\n",
    "splitdf=splitdf.withColumn(\"shortprof\",upper(substring(col(\"profession\"),1,3))).drop(\"profflag\")\n",
    "#Merging of column\n",
    "mergeddf=splitdf.select(col(\"custid\"),\"age\",concat_ws(\" \",col(\"firstname\"),col(\"lastname\")).alias(\"fullname\"),\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"shortprof\")#usage of select will help us avoid chaining of withColumn,drop,select\n",
    "display(mergeddf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03ef234-10ee-4a72-99b8-042a283d57fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######c. Typecasting & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2280d820-739d-4ed3-9337-8700f3384641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formatteddf=mergeddf.withColumn(\"datadt\",to_date(col(\"datadt\"),'yy/dd/MM'))#25/30/12 -> 2025-12-30\n",
    "formatteddf.printSchema()\n",
    "display(formatteddf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bbe034-7274-45e6-ae89-9f83243aeea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Customization - Application of Tailored Business specific Rules <br>\n",
    "a. User Defined Functions <br>\n",
    "b. Building of Frameworks & Reusable Functions (We will learn very next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16352ea-e98f-4494-b960-0e4c30429410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage3](stage3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "891c649e-f2e6-47a7-bd3d-0b6e5df58da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#formatteddf2=formatteddf.withColumn(\"sourcename\",upper(\"sourcename\"))\n",
    "#formatteddf2.show(2)\n",
    "#Caveat - If there is no upper() function is available already in spark dsl/sql, we can either search for some functions in the online opensource platform or we have to create one (custom functions)\n",
    "#from org.apache.sql.functions import upperodd\n",
    "\n",
    "def upperodd(colname_containsvalue):\n",
    "    convertedcolvalue=colname_containsvalue.upper()\n",
    "    return convertedcolvalue\n",
    "print(upperodd(\"irfan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1171b7b5-9dc5-4bfa-b179-584205ce90c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formatteddf2=rawdf1.withColumn(\"firstname\",upper(col(\"firstname\")))#we can't run python function as it is\n",
    "formatteddf2.explain()\n",
    "#display(formatteddf2.take(10))#prefer\n",
    "from pyspark.sql.functions import udf\n",
    "udfupper=udf(upperodd)#promote normal python function to spark ready udf\n",
    "formatteddf2=rawdf1.withColumn(\"firstname\",udfupper(col(\"firstname\")))#if udf is inevitable, then we create despite of performance bottleneck\n",
    "formatteddf2.explain()\n",
    "#display(formatteddf2.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232dd369-5bb8-4143-84b9-972754f8e724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####Create Python Custom Function with complex logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d9b849-607e-4852-9687-b41332caa867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculating age category from the given age of the customer\n",
    "def pythonAgeCat(dfcol):\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2508b396-ac62-4fa7-9bde-e3639f6c5bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Import udf library, Convert to UDF, Apply in the DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e0fa71-59b0-4f3d-9c51-e15e94aa1c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "sparkudfageCat=udf(pythonAgeCat)\n",
    "customdf=formatteddf.withColumn(\"agecat\",sparkudfageCat(\"age\"))\n",
    "display(customdf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b5336e-e2d8-4f5f-9ea7-688a6e997909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Data Curation/Processing - Applying different levels of business logics, transformation, filtering, grouping, aggregation and limits applying different transformation functions\n",
    "1. Select, Filter\n",
    "2. Derive flags & Columns\n",
    "3. Format\n",
    "4. Group & Aggregate\n",
    "5. Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3ad4e6-c860-4a04-95d4-68bfc2539e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Curation](stage4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e45a42-6262-41c2-9701-4d778d0a72fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Select, Filter\n",
    "In terms of Performance Optimzation - I ensured to do Push Down Optimization by doing select(project) & Filter(predicate) of what ever the expected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730cade4-00ca-4d79-843c-9da2ff2317d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select\n",
    "#select, functions, case, literal ,from,where,group by, having, order by, limit...\n",
    "#Select few columns by filtering few rows\n",
    "selectdf=customdf.select(\"custid\",\"age\",\"agecat\",col(\"profession\").alias(\"prof\"),\"agecat\")#DSL Select\n",
    "selectdf.show(5)\n",
    "selectdf=customdf.selectExpr(\"custid\",\"age\",\"agecat\",\"profession as prof\",\"agecat\")#SQL Select\n",
    "selectdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cac7ad4-6dd5-49d3-a486-1cb88111fa64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filter/Where - both are literally same (filter will be used by FBP developers & where will be used by SQL developers)\n",
    "filterdf=selectdf.filter((col(\"age\")>40) & (col(\"age\")<=60))#DSL operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf.where((col(\"age\")>40) & (col(\"age\")<=60))#DSL operation\n",
    "filterdf.show(5)\n",
    "\n",
    "filterdf=selectdf.filter(\"age>40 and age<=60\")#SQL where operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf.where(\"age>40 and age<=60\")#SQL where operation\n",
    "filterdf.show(5)\n",
    "#filterdf.write.saveAsTable(\"filtercust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef72bd0-f394-4294-8b0f-fa708a3ceb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####2. Derive flags & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c768480-658a-41ee-abe1-58676ee7cd8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We have created agecat using UDF (which is supposed to use only if it is inevitable)\n",
    "#But we can do the same using DSL When.otherwise or SQL CASE WHEN\n",
    "#Deriving Flag\n",
    "#Syntax in DSL: when(conditions,\"value\").when(conditions,\"value2\").otherwise(\"valuen\").alias(\"colname\")\n",
    "curateddf=customdf.select(\"*\",when(col(\"age\").isNull(),\"U\").when(col(\"age\")<=10,\"C\").when((col(\"age\")>10) & (col(\"age\")<=18),\"T\").when((col(\"age\")>18) & (col(\"age\")<=30),\"Y\").when((col(\"age\")>30) & (col(\"age\")<=50),\"M\").otherwise(\"S\").alias(\"agecatflag\"))#Suggessted than using UDFs\n",
    "display(curateddf.take(10))\n",
    "#Deriving Column\n",
    "#Syntax in SQL: case when conditions then value when conditions then value2 else valuen end as colname\n",
    "curateddf=curateddf.drop(\"agecat\").selectExpr(\"*\",\"\"\"case when age is null then 'Unknown' \n",
    "                              when age<=10 then 'child' \n",
    "                              when age>10 and age<=19 then 'teenager' \n",
    "                              when age>19 and age<=30 then 'young'\n",
    "                              when age>30 and age<=50 then 'middleaged'\n",
    "                              else 'oldaged' end as agecat\"\"\")#Suggessted than using UDFs\n",
    "display(curateddf.take(10))\n",
    "#Interview Answer of how you optimized the existing spark code developed by your ex team members?\n",
    "#I analysed the existing udfs used in my project and seeked for opportunities to convert them into SQL/dsl based programs by implementing the udf logics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cdc416-cf19-4fbd-8152-d079bcc37702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''def pythonAgeCat(dfcol):\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcbf946-d8a1-4445-bc60-7483a5d61a3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####3.Format (Deriving Columns with different format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a865263-ddfc-4343-a4a5-a02672621c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We can use different functions - string or number or date function for format modeling\n",
    "curateddf3=curateddf.select(\"*\",datediff(\"loaddt\",\"datadt\").alias(\"delaydays\"),year(\"datadt\").alias(\"datayear\")\n",
    "                            ,month(\"datadt\").alias(\"datamonth\")\n",
    "                            ,last_day(\"datadt\").alias(\"datalastday\")).withColumn(\"agecat\",initcap(\"agecat\"))\n",
    "display(curateddf3.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f85a4fa-d63b-48a0-88c2-5d4e225fc3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####4. Group & Aggregate\n",
    "Before performing grouping or aggr, consider the below factors from the dataset....<br>\n",
    "identifier?\tcid (high in cardinality/difference) (surrogate/naturalkey)<br>\n",
    "descriptive?\tname<br>\n",
    "metric?\tavg(age),count(distinct cid),max(age),min(age)<br>\n",
    "measure?\tage,cid<br>\n",
    "grouping?\tage,prof - low in cardinality/difference<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420761ee-e0b8-4982-b01b-56ace1471fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions.aggregate import avg,count,initcap,last_day,datediff,year,month,agg\n",
    "#What is the total number of customers we have?\n",
    "print(curateddf3.count())\n",
    "#What is the total number of customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\").count()\n",
    "display(curateddf4.take(100))\n",
    "#Multiple Aggregation with one grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\").avg(\"age\").withColumnRenamed(\"avg(age)\",\"avgage\")\n",
    "display(curateddf4.take(100))\n",
    "#To calculate multiple aggregation, we need to use a function called agg function\n",
    "curateddf4=curateddf3.groupBy(\"profession\").agg(count(\"custid\").alias(\"custcount\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(100))\n",
    "#curateddf4 this dataframe we materialize/store in some tables/files later\n",
    "#Multiple Aggregation with multiple grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\",\"agecat\").\\\n",
    "agg(count(\"custid\").alias(\"custcount\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5429817e-3991-4e84-9ce6-308f2232099b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####5. Ordering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeefcc8a-4f45-4f2d-98c5-adc45d6f2967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curateddf5=curateddf4.orderBy(\"profession\",\"agecat\")\n",
    "display(curateddf5.take(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4442baa3-2d25-452c-89ef-2547a7bd3bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####6. Limit\n",
    "Let us take an oppurtunity to understand different data limiting/restricting functions\n",
    "Limit is a dataframe TRANSFORMATION functions used to limit the number of rows returned in a spark dataframe FORMAT\n",
    "Take is a dataframe/RDD ACTION functions used to limit the number of rows returned in a python list FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a93323-8489-411b-a321-bb2f556fcbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#anything can be used under display()\n",
    "print(\"limit output\")\n",
    "curateddf5.limit(20).show(10)\n",
    "print(\"take output\")\n",
    "curateddf5.take(10)\n",
    "#When to use what\n",
    "#I have to filter some data in a limited dataset of 100 rows\n",
    "curateddf5.limit(100).filter(\"profession='Accountant'\").show()\n",
    "#curateddf5.take(100).filter(\"profession='Accountant'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffd70789-2f17-4757-919b-dd671bb9313c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##5. Data Wrangling - More of Analytics + Transformation\n",
    "  1. Joins - Relation/Connection established between one or more datasets/df/tabl to produce the broader/extended view of the data horizontally.\n",
    "  5 Categories of Joins:\n",
    "  inner, outer(left\n",
    "  right, full), self, cross, special optimized (semi, anti)\n",
    "  2. Lookup\n",
    "  3. Lookup & Enrichment\n",
    "  4. Schema Modeling  (Denormalization)\n",
    "  5. Windowing\n",
    "  6. Analytical\n",
    "  7. Set operations\n",
    "  8. grouping & aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4007e373-e29f-424c-9fd7-f5e587ebe9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage5](stage5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cd85a44-b7c0-4e5a-aea1-c5a803c707c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1. Joins\n",
    "Joins are Relation/connection of one or more tables to perform widened (horizontal) data analytics\n",
    "1. Frequently used simple joins (inner, left)\n",
    "2. InFrequent simple joins (self, right, full, cartesian)\n",
    "3. Advanced joins (Semi and Anti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec00c02c-1ecf-49c2-9b01-e1815933373e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#How to write join syntax in Spark and learn the semantics of join in spark\n",
    "rawdf1=spark.read.csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/staging/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1=rawdf1.na.drop().where(\"id<>'ten' and id<>'trailer_data:end of file'\")\n",
    "rawdf1.explain()\n",
    "rawdf1=rawdf1.where(\"id<>'ten' and id<>'trailer_data:end of file'\").na.drop()\n",
    "rawdf1.explain()\n",
    "leftdf=rawdf1.where(\"id in (4000100,4000101)\")\n",
    "rightdf=rawdf1.where(\"id in (4000100,4000102,4000103)\")\n",
    "leftdf.show(20,False)\n",
    "rightdf.show(20,False)\n",
    "innerjoindf=leftdf.join(rightdf,how='inner',on='id')\n",
    "innerjoindf.show(20)\n",
    "\n",
    "df3=rawdf1.where(\"id in (4000100,4000104)\")\n",
    "\n",
    "joindf2tables=leftdf.join(rightdf,how='inner',on='id')\n",
    "joindf3tables=joindf2tables.join(df3,how='left',on='id')\n",
    "joindf4tables=joindf3tables.join(df3,how='right',on='id')\n",
    "#innerjoindf.show(20)\n",
    "newjoin2=leftdf.join(rightdf,\"id\",\"inner\")\n",
    "display(newjoin2.show(20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b58137-dd3e-4fc4-8373-48f10638bcb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####Key Functions/Options learnt and used  till 2-Jan-2026 session\n",
    "######1)recursiveFileLookup,pathGlobFilter(used along saprk read)\n",
    "######2)union,unionByName(allowmissingcolumns='True'),mergeSchema\n",
    "######3)na.drop()\n",
    "         i)na.drop(how='any')\n",
    "         ii)na.fill()\n",
    "         iii)na.replace()\n",
    "######4)distinct()-#It will remove the row level duplicatesdropDuplicates()-#It will remove the column level duplicates\n",
    "######5)Standardization1 - Column Enrichment (Addition of columns)\n",
    "i)withColumn()\n",
    "ii)withColumnRenamed()\n",
    "iii)withColumnsRenamed({})\n",
    "iv)select  -DSL\n",
    "v)selectExpr-DSL+SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0148a4-cb2b-424a-a070-3961b7ce4097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
