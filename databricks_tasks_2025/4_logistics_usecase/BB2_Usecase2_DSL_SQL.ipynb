{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a1c79f7-78df-4dcd-afe1-29b65ed361cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Source Files has the Structured data with comma seperator (CSV) and Semistructured data (json)\n",
    "- Header, No comments is there in the data\n",
    "-Total columns are 5 in once source1 csv and 7 in the source2 csv.\n",
    "- Total columns in json are 11\n",
    "- Data Quality \n",
    "- - Null columns and null records are there\n",
    "- - duplicate rows & Duplicate id keys\n",
    "- - format issues are there\n",
    "- - Schema evolution is there\n",
    "- - Number of columns are more or less than the expected\n",
    "- - Mal data is found in the field value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2ad592-164a-41d9-8c71-33db5a54b3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "df_source1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_source1\")\n",
    "df_source2 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_source2\")\n",
    "#Analyse the schema, datatypes, columns etc.,\n",
    "#For Source file 1 we have the following schema\n",
    "\"\"\"shipment_id:string\n",
    "first_name:string\n",
    "last_name:string\n",
    "age:string\n",
    "role:string\"\"\"\"\"\n",
    "#For Source file 2 we have the following schema , which has 3 new fields added(role,hub_location,vehicle_type)\n",
    "\"\"\"shipment_id:string\n",
    "first_name:string\n",
    "last_name:string\n",
    "age:string\n",
    "role:string\n",
    "hub_location:string\n",
    "vehicle_type:string\"\"\"\n",
    "print(df_source1.columns)\n",
    "print(df_source2.columns)\n",
    "print(df_source1.dtypes)\n",
    "print(df_source2.dtypes)\n",
    "#Analyse the duplicate records count and summary of the dataframe.\n",
    "total_count_s1=df_source1.count()\n",
    "print(f\"The total count of records in source file 1 is {total_count_s1}\")\n",
    "total_count_s2=df_source2.count()\n",
    "print(f\"The total count of records in source file 2 is {total_count_s2}\")\n",
    "print(total_count_s2)\n",
    "distinct_count_s1=df_source1.distinct().count()\n",
    "print(f\"The total distinct count of records in source file 1 is {distinct_count_s1}\")\n",
    "distinct_count_s2=df_source2.distinct().count()\n",
    "print(f\"The total distinct count of records in source file 2 is {distinct_count_s2}\")\n",
    "df_source1.describe().show()\n",
    "df_source2.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f97700-cae1-4ab6-97e8-502ffbd1a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark=SparkSession.builder.appName(\"Logistics-Data\").getOrCreate()\n",
    "\n",
    "#Shipment IDs that appear in both master_v1 and master_v2\n",
    "print(\"Shipment IDs that appear in both master_v1 and master_v2\")\n",
    "common_shipments = df_source1.select(\"shipment_id\") \\\n",
    "    .intersect(df_source1.select(\"shipment_id\"))\n",
    "common_shipments.show()\n",
    "#shipment_id is non-numeric\n",
    "non_numeric_shipment = df_source1.filter(\n",
    "    ~col(\"shipment_id\").rlike(\"^[0-9]+$\")\n",
    ")\n",
    "\n",
    "non_numeric_shipment.show()\n",
    "\n",
    "#age is not an integer\n",
    "invalid_age = df_source1.filter(\n",
    "    ~col(\"age\").rlike(\"^[0-9]+$\")\n",
    ")\n",
    "\n",
    "invalid_age.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4604e706-d958-4e8b-8709-317f7f2ffcfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit, col,expr\n",
    "\n",
    "#Read both sources without enforcing schema\n",
    "df_source1 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_source1\")\n",
    ")\n",
    "df_source2 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_source2\")\n",
    ")\n",
    "\n",
    "#Align both to canonical schema\n",
    "df_system1 = (\n",
    "    df_source1\n",
    "    .select(\n",
    "        col(\"shipment_id\"),\n",
    "        col(\"first_name\"),\n",
    "        col(\"last_name\"),\n",
    "        expr(\"try_cast(age as int)\").alias(\"age\"),\n",
    "        col(\"role\")\n",
    "    )\n",
    "    .withColumn(\"data_source\", lit(\"system1\"))\n",
    ")\n",
    "\n",
    "df_system2 = (\n",
    "    df_source2\n",
    "    .select(\n",
    "        col(\"shipment_id\"),\n",
    "        col(\"first_name\"),\n",
    "        col(\"last_name\"),\n",
    "        expr(\"try_cast(age as int)\").alias(\"age\"),\n",
    "        col(\"role\"),\n",
    "        col(\"hub_location\"),\n",
    "        col(\"vehicle_type\")\n",
    "    )\n",
    "    .withColumn(\"data_source\", lit(\"system2\"))\n",
    ")\n",
    "\n",
    "df_canonical = df_system1.unionByName(df_system2, allowMissingColumns=True)\n",
    "df_canonical.printSchema()\n",
    "df_canonical.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0499a8aa-de73-4a62-9b97-7a2e37a53fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "df_clean_1 = df_canonical.dropna(subset=[\"shipment_id\", \"role\"])\n",
    "df_clean_1.show(10)\n",
    "#Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "df_clean_2 = df_clean_1.filter(\n",
    "    ~(col(\"first_name\").isNull() & col(\"last_name\").isNull())\n",
    ")\n",
    "df_clean_2.show(10)\n",
    "\n",
    "#Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "df_clean = df_clean_2.filter(col(\"shipment_id\").isNotNull())\n",
    "print(df_clean.count())\n",
    "df_check=df_clean.filter(col('shipment_id').isNull())\n",
    "print(df_check.count())\n",
    "\n",
    "#4. Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "df_scrub_1 = df_clean.na.fill({\"age\": -1})\n",
    "df_scrub_1.show(10)     \n",
    "#5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "df_scrub_2 = df_scrub_1.fillna({\"vehicle_type\": \"UNKNOWN\"})\n",
    "#6. Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "df_scrub_3 = df_scrub_2.withColumn( \"age\",when((col(\"age\") == \"ten\") | (col(\"age\") == \"\"), -1))\n",
    "    #.otherwise(col(\"age\"))\n",
    "#)\n",
    "df_scrub_3  = df_scrub_2.withColumn(\n",
    "    \"age\",\n",
    "    when(col(\"age\").rlike(\"^[0-9]+$\"), col(\"age\").cast(\"int\")).otherwise(-1)\n",
    ")\n",
    "#7. Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "df_final = df_scrub_3.replace({\n",
    "        \"truck\": \"LMV\",\n",
    "        \"bike\": \"TwoWheeler\"\n",
    "    },\n",
    "    subset=[\"vehicle_type\"]\n",
    ")\n",
    "df_final.show(10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Detail Dataframe creation <br>\n",
    "1. Read Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda7da10-c3a7-4678-86c7-66fc80a76e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_shipment_detail_json = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")   # use only if JSON is multiline\n",
    "    .json(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_shipment_detail_3000.json\")\n",
    ")\n",
    "\n",
    "df_shipment_detail_json.show(10)\n",
    "df_shipment_detail_json.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a41153-3ef7-47ba-98d0-c26ccab8ab60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb0898-71ad-470d-b516-5d2866e2a164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DecimalType\n",
    "#1. Add a column<br> \n",
    "df_detail_std = (\n",
    "    df_shipment_detail_json\n",
    "    .withColumn(\"domain\", lit(\"Logistics\"))\n",
    "    .withColumn(\n",
    "        \"shipment_date\",\n",
    "        coalesce(\n",
    "            try_to_date(col(\"shipment_date\"), \"yyyy-MM-dd\"),\n",
    "            try_to_date(col(\"shipment_date\"), \"yy-MM-dd\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"vehicle_type\", upper(\"vehicle_type\"))\n",
    "    .withColumn(\"source_city\", initcap(\"source_city\"))\n",
    ")\n",
    "#2. Column Uniformity: \n",
    "#csv file (Source1 and 2))\n",
    "df_master_std = df_final.withColumn(\n",
    "    \"role\",\n",
    "    lower(col(\"role\"))\n",
    ").withColumn(\n",
    "    \"vehicle_type\",\n",
    "    upper(col(\"vehicle_type\"))\n",
    ")\n",
    "#Json file \n",
    "\n",
    "\n",
    "df_detail_std.show(10)\n",
    "#3. Format Standardization:\n",
    "df_detail_std_nw = df_detail_std.withColumn(\n",
    "    \"shipment_date\",\n",
    "    to_date(col(\"shipment_date\"))\n",
    ")\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"shipment_cost\",\n",
    "    round(col(\"shipment_cost\"), 2).cast(DecimalType(10, 2))\n",
    ")\n",
    "#df_detail_std_nw.show(10)\n",
    "df_detail_std_nw.select(\"shipment_date\").show(truncate=False)\n",
    "df_detail_std_nw.printSchema()\n",
    "\n",
    "df_master_std.printSchema()\n",
    "df_detail_std.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bad2f2-b6d1-4c03-a1d3-d8cacee23a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "age: Cast String to Integer\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "shipment_weight_kg: Cast to Double\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "is_expedited: Cast to Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6053ee3a-61a0-4881-bb3d-dad7f77e3d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardizing column data types to fix schema drift and enable mathematical operations\n",
    "df_master_std = df_master_std.withColumn(\n",
    "    \"age\",\n",
    "    col(\"age\").cast(\"int\")\n",
    ")\n",
    "df_master_std.printSchema()\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"shipment_weight_kg\",\n",
    "    col(\"shipment_weight_kg\").cast(\"double\")\n",
    ")\n",
    "#is_expedited: Cast to Boolean<br> is not available in source file hence not used here\n",
    "df_detail_std.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3886f4be-4a05-49e8-a6b5-f388fa61a9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da8ed52-cf62-4912-a6fe-104879d00190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_master_std = (\n",
    "    df_master_std\n",
    "    .withColumnRenamed(\"first_name\", \"staff_first_name\")\n",
    "    .withColumnRenamed(\"last_name\", \"staff_last_name\")\n",
    "    .withColumnRenamed(\"hub_location\", \"origin_hub_city\")\n",
    ")\n",
    "df_master_std.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dee4d2b-49cd-4578-8e9b-7dc4be0c3972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32c53c8-746e-4523-81af-ac85e811902a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_master_sel = df_master_std.select(\n",
    "    \"shipment_id\",\n",
    "    \"staff_first_name\",\n",
    "    \"staff_last_name\",\n",
    "    \"role\",\n",
    "    \"origin_hub_city\"\n",
    ")\n",
    "\n",
    "df_detail_sel = df_detail_std.select(\n",
    "    \"shipment_id\",\n",
    "    \"shipment_cost\"\n",
    ")\n",
    "df_final = (\n",
    "    df_master_sel\n",
    "    .join(df_detail_sel, on=\"shipment_id\", how=\"left\")\n",
    ")\n",
    "df_final = df_final.withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    current_timestamp()\n",
    ")\n",
    "df_final = df_final.select(\n",
    "    \"shipment_id\",\n",
    "    \"staff_first_name\",\n",
    "    \"staff_last_name\",\n",
    "    \"role\",\n",
    "    \"origin_hub_city\",\n",
    "    \"shipment_cost\",\n",
    "    \"ingestion_timestamp\"\n",
    ")\n",
    "df_final.printSchema()\n",
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b14cbad-85d9-457b-86fb-7da80534a20a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Record level duplicate check\n",
    "#df_record_dedup = df_final.dropDuplicates()\n",
    "#df_record_dedup.count()\n",
    "#Column level duplicate check\n",
    "df_col_dedup = df_final.dropDuplicates([\"shipment_id\"])\n",
    "df_col_dedup.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ecd12-8081-4c8d-925e-d47c84e520d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe36f2c-be11-48a6-863c-f455238c2b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df140c52-2b22-43fb-9dd2-fea3b3120f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add Audit Timestamp (load_dt) Source File: logistics_source1 and logistics_source2\n",
    "#Create Full Name (full_name) Source File: logistics_source1 and logistics_source2\n",
    "df_source1 = df_source1.withColumn(\n",
    "    \"full_name\",\n",
    "    concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))\n",
    ")\n",
    "df_source2 = df_source2.withColumn(\n",
    "    \"full_name\",\n",
    "    concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))\n",
    ")\n",
    "df_source1.select(\"first_name\", \"last_name\", \"full_name\").show(3)\n",
    "df_source2.select(\"first_name\", \"last_name\", \"full_name\").show(3)\n",
    "df_source1_audit = df_source1.withColumn(\n",
    "    \"load_dt\",\n",
    "    current_timestamp()\n",
    ")\n",
    "df_source2_audit = df_source2.withColumn(\n",
    "    \"load_dt\",\n",
    "    current_timestamp()\n",
    ")\n",
    "df_source1_audit.printSchema()\n",
    "df_source2_audit.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd064cd-251a-4b6c-938a-48e3240ee397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f8af2e-b649-4e2b-8fe8-40257cca3199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define Route Segment (route_segment) \n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"route_segment\",\n",
    "    concat_ws(\"-\", col(\"source_city\"), col(\"destination_city\"))\n",
    ")\n",
    "df_detail_std.select(\"route_segment\").show(5)\n",
    "\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"vehicle_identifier\",\n",
    "    concat_ws(\"_\", col(\"vehicle_type\"), col(\"shipment_id\"))\n",
    ")\n",
    "df_detail_std.select(col('vehicle_identifier')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa6d0ba-f205-4185-9c1e-9d4dffd58aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Derive shipment_year\n",
    "df_detail_std=df_detail_std.withColumn(\"shipment_year\",year(col(\"shipment_date\")))\n",
    "df_detail_std.select(\"shipment_date\",\"shipment_year\").show(5)\n",
    "\n",
    "#Derive shipment_month\n",
    "df_detail_std=df_detail_std.withColumn(\"shipment_month\",month(col(\"shipment_date\")))\n",
    "df_detail_std.select(\"shipment_date\",\"shipment_month\").show(5)\n",
    "\n",
    "#Flag Weekend Operations (is_weekend)\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"is_weekend\",\n",
    "    when(dayofweek(col(\"shipment_date\")).isin(1, 7), True)\n",
    "    .otherwise(False)\n",
    ")\n",
    "df_detail_std .select (\"shipment_date\",\"shipment_year\",\"shipment_month\",\"is_weekend\").show(5)\n",
    "df_detail_std .select (\"shipment_date\",\"shipment_year\",\"shipment_month\",\"is_weekend\").filter(col(\"is_weekend\")==True).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af11b65-b89e-4b83-bf2d-0d93705c6ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Calculate Unit Cost (cost_per_kg)\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"cost_per_kg\",\n",
    "    when(\n",
    "        (col(\"shipment_weight_kg\").isNotNull()) & (col(\"shipment_weight_kg\") > 0),\n",
    "        round(col(\"shipment_cost\") / col(\"shipment_weight_kg\"), 2)\n",
    "    ).otherwise(None)\n",
    ")\n",
    "#2. Track Shipment Age (days_since_shipment)\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"days_since_shipment\",\n",
    "    datediff(current_date(), col(\"shipment_date\"))\n",
    ")\n",
    "#3. Compute Tax Liability (tax_amount)\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"tax_amount\",\n",
    "    round(col(\"shipment_cost\") * 0.18, 2)\n",
    ")\n",
    "\n",
    "df_detail_std.select(\n",
    "    \"shipment_cost\",\n",
    "    \"shipment_weight_kg\",\n",
    "    \"cost_per_kg\",\n",
    "    \"days_since_shipment\",\n",
    "    \"tax_amount\"\n",
    ").show(5, truncate=False)\n",
    "#Add Audit Timestamp (load_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8552e2da-2067-41a2-b215-510d00d3fedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Remove Redundant Name Columns\n",
    "df_source1 = df_source1.drop(\"first_name\", \"last_name\")\n",
    "df_source2 = df_source2.drop(\"first_name\", \"last_name\")\n",
    "df_source1.printSchema()\n",
    "df_source2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c7749d-294b-45cf-b1e7-df6cd3c5234e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Splitting (Extraction) \n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"order_prefix\",\n",
    "    regexp_extract(col(\"order_id\"), \"^[A-Za-z]+\", 0)\n",
    ").withColumn(\n",
    "    \"order_sequence\",\n",
    "    regexp_extract(col(\"order_id\"), \"[0-9]+$\", 0)\n",
    ")\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"ship_year\",\n",
    "    year(col(\"shipment_date\"))\n",
    ").withColumn(\n",
    "    \"ship_month\",\n",
    "    month(col(\"shipment_date\"))\n",
    ").withColumn(\n",
    "    \"ship_day\",\n",
    "    dayofmonth(col(\"shipment_date\"))\n",
    ")\n",
    "df_detail_std.select(\"order_id\",\"order_prefix\",\"order_sequence\",\"ship_year\",\"ship_month\",\"ship_day\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed59b3b-b33a-4fe9-b62f-79a9a7aac76a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8316a6d-d65f-45d8-ba89-2e376fc6b032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Merging (Concatenation)\n",
    "df_detail_std = df_detail_std.withColumn(\n",
    "    \"route_lane\",\n",
    "    concat_ws(\"->\", col(\"source_city\"), col(\"destination_city\"))\n",
    ")\n",
    "df_detail_std .select( \"order_id\",\n",
    "    \"order_prefix\",\n",
    "    \"order_sequence\",\n",
    "    \"shipment_date\",\n",
    "    \"ship_year\",\n",
    "    \"ship_month\",\n",
    "    \"ship_day\",\n",
    "    \"route_lane\"\n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8053710-4f96-429c-a27b-91e9ca618d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **UDF1: Complex Incentive Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "462b625e-b4f3-46d0-85b2-8654399cad04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Python function for calculate bonus:\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def calculate_bonus(role, age, salary):\n",
    "    if role == \"driver\" and age is not None and age >= 0:\n",
    "        if age > 50:\n",
    "            return salary * 0.15\n",
    "        elif age < 30:\n",
    "            return salary * 0.05\n",
    "    return 0.0\n",
    "    \n",
    "#converting the function to udf :\n",
    "calculate_bonus_udf = udf(calculate_bonus, DoubleType())\n",
    "df_canonical.select(\n",
    "    col(\"role\"),\n",
    "    lower(trim(col(\"role\"))).alias(\"clean_role\"),\n",
    "    col(\"age\")\n",
    ").distinct().show(truncate=False)\n",
    "\n",
    "df_with_bonus = df_canonical.withColumn(\n",
    "    \"projected_bonus\",\n",
    "    calculate_bonus_udf(\n",
    "        lower(col(\"role\")),\n",
    "        col(\"age\"),\n",
    "        lit(15000)\n",
    "    )\n",
    ")\n",
    "#Validate\n",
    "df_with_bonus.select(\n",
    "    \"role\",\n",
    "    \"age\",\n",
    "    \"projected_bonus\"\n",
    ").where(col(\"role\") == \"driver\") \\\n",
    " .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f730a479-8a33-421a-a931-22aa90826477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **UDF2: PII Masking (Privacy Compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94529160-f578-4679-a6f2-09e381428034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_masked = df_source1.withColumn(\n",
    "    \"masked_name\",\n",
    "    concat(\n",
    "        substring(col(\"full_name\"), 1, 2),       # First 2 chars\n",
    "        lit(\"****\"),                              # Mask\n",
    "        substring(col(\"full_name\"), length(col(\"full_name\")), 1)  # Last char\n",
    "    )\n",
    ")\n",
    "\n",
    "df_masked1 = df_source1.withColumn(\n",
    "    \"masked_name\",\n",
    "    when(\n",
    "        length(col(\"full_name\")) >= 3,\n",
    "        concat(\n",
    "            substring(col(\"full_name\"), 1, 2),\n",
    "            lit(\"****\"),\n",
    "            substring(col(\"full_name\"), length(col(\"full_name\")), 1)\n",
    "        )\n",
    "    ).otherwise(col(\"full_name\"))\n",
    ")\n",
    "\n",
    "df_masked.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\"  **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending) then `priority_flag` (Descending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65d9f75-5b0f-4c12-946f-34866587304e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1. Select (Projection)** <br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee776cab-e186-4a32-8646-fbe7851932de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select (Projection)\n",
    "\n",
    "df_projection = df_canonical.select(\n",
    "    col(\"first_name\"),\n",
    "    col(\"role\"),\n",
    "    col(\"hub_location\")\n",
    ").filter(col(\"hub_location\").isNotNull())\n",
    "df_projection.show(5,truncate=False)\n",
    "df_projection.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346e6ba9-37e4-4c62-bfe0-ab0ab4ba14c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **2. Filter (Selection)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97694bd7-e12e-456a-bb75-31597416faa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_operational_issues = df_detail_std.filter(\n",
    "    col(\"shipment_status\").isin(\"DELAYED\", \"RETURNED\")\n",
    ")\n",
    "df_operational_issues.show(truncate=False)\n",
    "df_senior_staff = df_canonical.filter(col(\"age\") > 50)\n",
    "df_senior_staff.select(\n",
    "    \"shipment_id\",\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"age\",\n",
    "    \"role\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4ad217-010d-4314-b05d-fe571dbf4734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe68de6-3742-4360-8646-a9491571c8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_detail_flagged = df_detail_std.withColumn(\n",
    "    \"is_high_value\",\n",
    "    when(col(\"shipment_cost\") > 50000, True).otherwise(False)\n",
    ")\n",
    "df_detail_flagged = df_detail_flagged.withColumn(\n",
    "    \"is_weekend\",\n",
    "    when(dayofweek(col(\"shipment_date\")).isin(1, 7), True).otherwise(False)\n",
    ")\n",
    "df_detail_flagged.select(\n",
    "    \"shipment_id\",\n",
    "    \"shipment_date\",\n",
    "    \"shipment_cost\",\n",
    "    \"is_high_value\",\n",
    "    \"is_weekend\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a94953e-f6a1-41fa-9aaf-f40e2b6f69cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **4. Format (Standardization)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\"  **\"CHENNAI\"**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6cf20bb-e4ab-4c6f-bff8-5f26ca904d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_detail_fmt = df_detail_std.withColumn(\n",
    "    \"shipment_cost_fmt\",\n",
    "    concat(lit(\"\"), format_number(\"shipment_cost\", 2))\n",
    ")\n",
    "df_detail_fmt = df_detail_fmt.withColumn(\n",
    "    \"source_city\",\n",
    "    upper(\"source_city\")\n",
    ")\n",
    "df_detail_fmt.select(\n",
    "    \"shipment_id\",\n",
    "    \"shipment_cost\",\n",
    "    \"shipment_cost_fmt\",\n",
    "    \"source_city\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ba8b7a-cbba-4ff2-999d-1f90c332843b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e7af7c-4d96-42d5-b469-03686f413ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Aggreagtion\n",
    "df_staff_by_hub = (\n",
    "    df_canonical\n",
    "    .groupBy(\"hub_location\")\n",
    "    .agg(count(\"*\").alias(\"staff_count\"))\n",
    ")\n",
    "\n",
    "df_staff_by_hub.show(5,truncate=False)\n",
    "df_staff_by_hub.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e265c3-36cf-4af5-b8ad-2b87becf696c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **6. Sorting (Ordering)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending) then `shipment_cost` (Descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6e8662-8e5e-4a6d-8f6d-f2ff092da173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sorted_by_cost = df_detail_std.orderBy(\n",
    "    col(\"shipment_cost\").desc()\n",
    ")\n",
    "df_sorted_by_cost.select(\n",
    "    \"shipment_id\",\n",
    "    \"shipment_cost\"\n",
    ").show(5,truncate=False)\n",
    "\n",
    "df_dispatch_schedule = df_detail_std.orderBy(\n",
    "    col(\"shipment_date\").asc(),\n",
    "    col(\"shipment_cost\").desc()\n",
    ")\n",
    "df_dispatch_schedule.select(\n",
    "    \"shipment_id\",\n",
    "    \"shipment_date\"    \n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1bcf59-e354-474b-a00d-07e77150d59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **7. Limit (Top-N Analysis)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c623abb6-b3bf-4096-943e-7c54214572a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_top_delayed = (\n",
    "    df_detail_std\n",
    "    .filter(col(\"shipment_status\") == \"DELAYED\")\n",
    "    .orderBy(col(\"shipment_cost\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "df_top_delayed.select(\n",
    "    \"shipment_id\",\n",
    "    \"shipment_status\",\n",
    "    \"shipment_cost\",\n",
    "    \"shipment_date\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`.\n",
    "\n",
    "### **2. Lookup**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the corporate `Master_City_List`.\n",
    "* **Action:** Compare values against a reference list.\n",
    "\n",
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`.\n",
    "\n",
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment.\n",
    "\n",
    "### **7. Set Operations**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n",
    "\n",
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f83799-7290-4dac-bb02-cb96f9d523ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> logistics_shipment_detail_3000.json<br>\n",
    "##### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f8a535-6d29-460e-976e-9180345c6df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_master_clean = (\n",
    "    df_master_std\n",
    "    .filter(col(\"shipment_id\").rlike(\"^[0-9]+$\"))\n",
    "    .withColumn(\"shipment_id\", col(\"shipment_id\").cast(\"string\"))\n",
    ")\n",
    "df_master_clean .printSchema()\n",
    "df_detail_clean = (\n",
    "    df_detail_std\n",
    "    .filter(col(\"shipment_id\").rlike(\"^[0-9]+$\"))\n",
    "    .withColumn(\"shipment_id\", col(\"shipment_id\").cast(\"string\"))\n",
    ")\n",
    "df_staff_shipments_inner = (\n",
    "    df_master_clean.alias(\"s\")\n",
    "    .join(\n",
    "        df_detail_clean.alias(\"d\"),\n",
    "        on=col(\"s.shipment_id\") == col(\"d.shipment_id\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "df_staff_shipments_inner.select(\n",
    "    \"s.shipment_id\",\n",
    "    \"s.staff_first_name\",\n",
    "\t\"s.staff_last_name\",   \n",
    "    \"s.role\",\n",
    "    \"d.shipment_status\",\n",
    "    \"d.shipment_cost\"\n",
    ").show(5,truncate=False)\n",
    "\n",
    "#B. Left Join  Idle Resource Check\n",
    "df_staff_shipments_left = (\n",
    "    df_master_clean.alias(\"s\")\n",
    "    .join(\n",
    "        df_detail_clean.alias(\"d\"),\n",
    "        on=col(\"s.shipment_id\") == col(\"d.shipment_id\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "#Filter Idle Staff\n",
    "df_idle_staff = df_staff_shipments_left.filter(\n",
    "    col(\"d.shipment_id\").isNull()\n",
    ")\n",
    "\n",
    "df_idle_staff.select(\n",
    "    \"s.shipment_id\",\n",
    "    \"s.staff_first_name\",\n",
    "\t\"s.staff_last_name\", \n",
    "    \"s.role\",\n",
    "    \"s.origin_hub_city\"\n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fa30f0-e2ac-4a5a-b196-52cbc9d66d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c86f5c-1b92-459f-b4a0-08153e828f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 Infrequent Simple Joins\n",
    "staff_a = df_master_clean.alias(\"a\")\n",
    "staff_b = df_master_clean.alias(\"b\")\n",
    "df_peer_staff = (\n",
    "    staff_a\n",
    "    .join(\n",
    "        staff_b,\n",
    "        col(\"a.origin_hub_city\") == col(\"b.origin_hub_city\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .filter(col(\"a.shipment_id\") != col(\"b.shipment_id\"))\n",
    ")\n",
    "df_peer_staff.select(\n",
    "    col(\"a.staff_first_name\").alias(\"staff_A\"),\n",
    "    col(\"b.staff_first_name\").alias(\"staff_B\"),\n",
    "    col(\"a.origin_hub_city\")\n",
    ").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599133fe-ff02-4cbd-9ab8-9f02b8d4a721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **1.3 Advanced Joins (Semi and Anti)**\n",
    "**Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d539f4e7-8e1b-4ec2-8701-913adf2b81cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_with_shipments = (\n",
    "  df_master_clean\n",
    "    .join(df_detail_clean, on=\"shipment_id\", how=\"left_semi\")\n",
    ")\n",
    "drivers_with_shipments.show(5)\n",
    "\n",
    "drivers_without_shipments = (\n",
    "    df_master_clean\n",
    "    .join(df_detail_clean, on=\"shipment_id\", how=\"left_anti\")\n",
    ")\n",
    "drivers_without_shipments.show(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d22b17-21a5-4e65-a181-e439e47484d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **2. Lookup**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the corporate `Master_City_List`.\n",
    "* **Action:** Compare values against a reference list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a0c4bd-ec70-4886-8ae2-690fc9d5ab0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_city_master = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/Master_City_List.csv\")\n",
    "\n",
    "staff_df_clean = df_source1.unionByName(df_source2,allowMissingColumns=True)\n",
    "\n",
    "df_city_master = df_city_master.withColumn(\n",
    "    \"city_name\",\n",
    "    upper(trim(col(\"city_name\")))\n",
    ")\n",
    "valid_hubs_df = staff_df_clean .join(\n",
    "    df_city_master,\n",
    "    staff_df_clean.hub_location == df_city_master.city_name,\n",
    "    how=\"left_semi\"\n",
    ")\n",
    "valid_hubs_df.show(2,truncate=False)\n",
    "invalid_hubs_df = staff_df_clean .join(\n",
    "    df_city_master,\n",
    "    staff_df_clean .hub_location == df_city_master.city_name,\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "invalid_hubs_df.show(2,truncate=False)\n",
    "#staff_df_clean.filter(col(\"hub_location\").isNotNull()).show(2,truncate=False)\n",
    "#df_city_master .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44b38fa-06c5-4303-8103-76a85973f7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **3. Lookup & Enrichment**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding `lat` and `long` columns for map plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c3c197-a8e2-421d-b0b8-cda44a1c48f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_city_master = df_city_master.withColumn(\n",
    "    \"city_name\", upper(trim(col(\"city_name\")))\n",
    ")\n",
    "staff_geo_df = (\n",
    "    staff_df_clean\n",
    "    .join(\n",
    "        df_city_master,\n",
    "        col(\"hub_location\") == col(\"city_name\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "staff_geo_df.filter(col(\"hub_location\").isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c535dc-518a-4d38-bd69-94ef37654704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_detail_std.printSchema()\n",
    "#df_city_master.printSchema()\n",
    "staff_df_clean .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4616ab82-9a14-4403-affa-3bc242ed3398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3025cf-366d-48bb-b7bb-f3c6acf0d473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_master_clean = (\n",
    "    df_master_std\n",
    "    .withColumn(\"shipment_id\", col(\"shipment_id\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "df_detail_clean = (\n",
    "    df_detail_std\n",
    "    .withColumn(\"shipment_id\", col(\"shipment_id\").cast(\"string\"))\n",
    ")\n",
    "df_master_clean.printSchema()\n",
    "df_detail_clean.printSchema()\n",
    "#Denormalize (Correct Join Direction)\n",
    "wide_shipment_history = (\n",
    "    df_detail_clean.alias(\"d\")\n",
    "    .join(\n",
    "        df_master_clean.alias(\"m\"),\n",
    "        col(\"d.shipment_id\") == col(\"m.shipment_id\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "#wide_shipment_history.show(5)\n",
    "wide_shipment_history = wide_shipment_history.select(\n",
    "    # Key\n",
    "    \"d.shipment_id\",\n",
    "    # Detail-level facts\n",
    "    \"m.staff_first_name\",\n",
    "    \"m.staff_last_name\",\n",
    "\t\"m.age\",\n",
    "    \"d.shipment_status\",\n",
    "    \"d.cargo_type\",\n",
    "\n",
    "    # Master attributes\n",
    "    \"m.vehicle_type\",\n",
    "\t\"d.shipment_cost\",\n",
    "    \"m.origin_hub_city\",\n",
    "    \"d.source_city\"\n",
    "   ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "324e14ea-304b-41d5-ba49-0f208c89843f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b760c8-b267-433d-a887-8105baf4259c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank, row_number\n",
    "df_source2 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_source2\")\n",
    ")\n",
    "\n",
    "df_shipment_detail_json = (\n",
    "    spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .json(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_shipment_detail_3000.json\")\n",
    ")\n",
    "df_source2_clean = (\n",
    "    df_source2\n",
    "    .withColumn(\"shipment_id\", col(\"shipment_id\").cast(\"string\"))\n",
    "   \n",
    ")\n",
    "df_shipment_detail_flat = (\n",
    "    df_shipment_detail_json\n",
    "    .withColumn(\"shipment_id\", col(\"shipment_id\").cast(\"string\"))\n",
    "    .select(\n",
    "        col(\"shipment_id\"),\n",
    "        col(\"shipment_cost\"),\n",
    "        col(\"shipment_date\")\n",
    "    )\n",
    ")\n",
    "base_df = (\n",
    "    df_shipment_detail_flat.alias(\"d\")\n",
    "    .join(\n",
    "        df_source2_clean.alias(\"m\"),\n",
    "        col(\"d.shipment_id\") == col(\"m.shipment_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "base_df.printSchema()\n",
    "\n",
    "driver_cost_df = (\n",
    "    base_df\n",
    "    .groupBy(\"hub_location\")\n",
    "    .agg(\n",
    "        sum(\"shipment_cost\").alias(\"total_shipment_cost\")\n",
    "    )\n",
    ")\n",
    "\n",
    "hub_window = Window.partitionBy(\"hub_location\") \\\n",
    "                   .orderBy(col(\"total_shipment_cost\").desc())\n",
    "\n",
    "ranked_df = (\n",
    "    driver_cost_df\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(hub_window))\n",
    "    .withColumn(\"row_number\", row_number().over(hub_window))\n",
    ")\n",
    "top_3_dense = ranked_df.filter(col(\"dense_rank\") <= 3)\n",
    "top_3_dense.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97076d2e-d2d5-4b98-9f55-458cd91d8582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac884d25-c2db-445d-9e2d-eb5c25d8b433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_df = base_df.withColumn(\n",
    "    \"shipment_date\",\n",
    "    to_date(col(\"shipment_date\"))\n",
    ")\n",
    "\n",
    "driver_window = Window.partitionBy(\"m.shipment_id\") \\\n",
    "                      .orderBy(col(\"shipment_date\"))\n",
    "base_df.printSchema()\n",
    "idle_time_df = (\n",
    "    base_df\n",
    "    .withColumn(\n",
    "        \"prev_shipment_date\",\n",
    "        lag(\"shipment_date\").over(driver_window)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"idle_days\",\n",
    "        datediff(\n",
    "            col(\"shipment_date\"),\n",
    "            col(\"prev_shipment_date\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "idle_time_df.filter(col(\"idle_days\").isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24f5620-a71b-432f-b937-7965af467b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **7. Set Operations**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1ec2cd-91d1-4370-9c3c-85828ec2715b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_source1 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_source1\")\n",
    ")\n",
    "df_source2 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/logistics/logistics_source2\")\n",
    ")\n",
    "staff_df = df_source1.unionByName(df_source2,allowMissingColumns=True)\n",
    "staff_df.show(5)\n",
    "#Identify IDs present in BOTH Source1 and Source2\n",
    "common_staff_ids = (\n",
    "    df_source1\n",
    "    .select(\"shipment_id\")\n",
    "    .intersect(\n",
    "        df_source2.select(\"shipment_id\")\n",
    "    )\n",
    ")\n",
    "common_staff_ids.show(5)\n",
    "#Identify  IDs present in Source2 but missing from Source1\n",
    "new_miss_src = (\n",
    "    df_source2\n",
    "    .select(\"shipment_id\")\n",
    "    .exceptAll(\n",
    "        df_source1.select(\"shipment_id\")\n",
    "    )\n",
    ")\n",
    "new_miss_src.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9752a282-1015-4e04-963a-3991864e3bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c0a3d1-c771-4f06-a36f-a10862e52be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "base_df = (\n",
    "    df_shipment_detail_flat.alias(\"d\")\n",
    "    .join(\n",
    "        df_source2.alias(\"m\"),\n",
    "        col(\"d.shipment_id\") == col(\"m.shipment_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "base_df.show(5)\n",
    "rollup_df = (\n",
    "    base_df\n",
    "    .rollup(\"hub_location\", \"vehicle_type\")\n",
    "    .agg(\n",
    "        sum(\"shipment_cost\").alias(\"total_shipment_cost\")\n",
    "    )\n",
    ")\n",
    "rollup_df.show(5)\n",
    "cube_df = (\n",
    "    base_df\n",
    "    .cube(\"hub_location\", \"vehicle_type\")\n",
    "    .agg(\n",
    "        sum(\"shipment_cost\").alias(\"total_shipment_cost\")\n",
    "    )\n",
    ")\n",
    "cube_df.show(5)\n",
    "final_df = cube_df.withColumn(\n",
    "    \"aggregation_level\",\n",
    "    when(col(\"hub_location\").isNull() & col(\"vehicle_type\").isNull(), \"GRAND_TOTAL\")\n",
    "    .when(col(\"vehicle_type\").isNull(), \"HUB_TOTAL\")\n",
    "    .when(col(\"hub_location\").isNull(), \"VEHICLE_TOTAL\")\n",
    "    .otherwise(\"HUB_VEHICLE_TOTAL\")\n",
    ")\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase2_DSL_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
