{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee99bb57-cc17-4aaa-836a-89875ba34790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Try to use this sales data with atleast very important and important options...\n",
    "https://drive.google.com/file/d/1MZI4XIofL-0QpMIr9sFODSKVkexrSZ1A/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2885f6-7f1c-451d-98fe-b0e78cf49bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_path=\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/sales.csv\"\n",
    "df_sales=spark.read.csv(sales_path,header=True,inferSchema=True,sep=\",\",comment=\"#\",nullValue=\"na\",modifiedAfter='2025-12-18',dateFormat=\"yyyy-dd-MM\",ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,quote='\"',nanValue=-1 ,escape=\"\\\\\")\n",
    "df_sales.show(5)\n",
    "display(df_sales)\n",
    "#nan value works on float/double columns only not on numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4280bcb1-491e-43e6-a978-4c026e7624fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. CSV Advanced Features - \n",
    "######Very Important - path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None,header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, \n",
    "######Important - mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None,  quote: Optional[str]=None, escape: Optional[str]=None, \n",
    "Not Important but good to know once - encoding: Optional[str]=None, comment: Optional[str]=None,ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None,   multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a070bc4-17e0-4059-97ef-0e29d09c8cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A. Options for handling quotes & Escape\n",
    "\n",
    "id,name,remarks\n",
    "1,'Ramesh, K.P','Good performer'\n",
    "2,'Manoj','Needs ~'special~' attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aeddbb0-2538-47dd-91cf-2b99f09f172b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When to go for quote: If the data is having delimiter in it..\n",
    "#When to go for escape: If the data is having quote in it...\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/malformed_data.txt\",header=False,sep=',',mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote='\"',escape=\"|\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2878e608-057e-4b0d-a33d-3ad862025f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B. Comments, Multi line, leading and trailing whitespace handling, null and nan handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6006eaa9-ef12-4ed3-a3a7-c665bcafbf5f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766113838409}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "struct1=\"custid int,name string,height float,joindt date,age string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata2.txt\",header=False,mode='permissive'\n",
    "                                   ,multiLine=True,quote=\"'\",ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,\n",
    "                                   nullValue='na',nanValue=-1,maxCharsPerColumn='100',modifiedAfter='2025-12-19',dateFormat=\"yyyy-dd-MM\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a395acd-7233-49e8-806d-800d56de7195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C. Read modes in csv (Important feature)\n",
    "If any data challenges (malformed data) such as format issue/column numbers (lesser/more than expected) issue etc.,\n",
    "### There are 3 typical read modes and the default read mode is permissive.\n",
    "##### 1. permissive — All fields are set to null and corrupted records are placed in a string column called _corrupt_record\n",
    "##### \t2. dropMalformed — Drops all rows containing corrupt records.\n",
    "##### 3. failFast — Fails when corrupt records are encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7624274-7e89-4410-8be3-1e573ce99bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We learned about few important features mode, columnNameOfCorruptRecord, Quote, Comment\n",
    "#Question - Corrupt_record column consume more memory because it capturing all the column values(incorrect) in one column. ? Useful for doing RCA (Root Cause Analysis/Debugging)\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/malformeddata.txt\",header=False,sep=',',mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/malformeddata.txt\",header=False,sep=',',mode='dropMalformed',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/source_dir/malformeddata.txt\",header=False,sep=',',mode='failFast',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c793a1ba-d061-45d9-852d-88a5598cd125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####5. Reading data from other formats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c69af82-91aa-4854-b6ce-b1eb57c70629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Reading csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4039ed-9dec-418d-b54b-a2fcfbd0000a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.options(inferSchema='true',sep='~',header='true').csv(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/csvout\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748f2729-c766-4363-ab86-b4d73205c76c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. JSON Advanced Features - \n",
    "**Very Important** - path,schema,columnNameOfCorruptRecord,dateFormat,timestampFormat,multiLine,pathGlobFilter,recursiveFileLookup<br>\n",
    "No header, No inferSchema, No sep in json...<br>\n",
    "**Important** - primitivesAsString(don't do inferSchema), prefersDecimal, allowComments, allowUnquotedFieldNames, `allowSingleQuotes`, lineSep, samplingRatio, dropFieldIfAllNull, modifiedBefore, modifiedAfter, useUnsafeRow(This is performance optimization when the data is loaded into spark memory) <br>\n",
    "**Not Important** (just try to know once for all) - allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, allowUnquotedControlChars, encoding, locale, allowNonNumericNumbers<br>%md\n",
    "####2. Reading json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f1957d-69c5-4d4e-b452-99bb5683cc24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.json(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/jsonout\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d574b0b-96fe-4590-859f-00900b0bffbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option\n",
    "#primitivesAsString = inferSchema=False\n",
    "dfjson1=spark.read.json(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/jsonout\",primitivesAsString=True)\n",
    "dfjson1.printSchema()\n",
    "#prefersDecimal\n",
    "dfjson1=spark.read.json(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/jsonout\",prefersDecimal=True)\n",
    "dfjson1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5df82fe-767f-4ab1-a316-15e9a3e57f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Reading xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35a6186-f61a-4c61-9f5f-c5f303fe6b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.xml(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/xmlout\",rowTag=\"cust\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26a3f08-51ab-4b48-89e8-0d077a3becd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading serialized data (orc/parquet/delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728496da-e9f0-4a67-a508-eb05c646bc22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3. Serialized data Advanced Feature - orc, parquet/delta (very very important & we learn indepth)\n",
    "- PathOrPaths\n",
    "- **mergeSchema** - Important interview property (make it proactive/make it driven in the interview) SCHEMA EVOLUTION\n",
    "- pathGlobFilter\n",
    "- recursiveFileLookup\n",
    "- modifiedBefore\n",
    "- modifiedAfter\n",
    "Problem statement:\n",
    "Source is sending data in any way they want...\n",
    "Day1/source1- 5 cols\n",
    "Day2/source2 - 7 Cols\n",
    "\n",
    "1. I am reading the dataframe in csv/json...\n",
    "2. Writing into a orc/parquet format in a single location.\n",
    "3. Reading data in a orc/parquet format using mergeSchema option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f50aaac-de09-4595-875d-e859d874cb42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Case study: If the source (external) is sending data to us, if our consumer (Datascience/Dataanalytics) is directly communicated with our source system and asked them to propogate more/less attributes/features without the knowledge of the Dataengineering team? Here we have implement the strategy of Schema Evolution using option mergeSchema\n",
    "#Story building for interview: We get product data from source which get evolved on a frequent basis for eg. product data originally sent without gendra, costprice, purchaseprice, profit/loss metrics, demant information...\n",
    "#Steps to follow:\n",
    "#1. Collect the data as it is from the source\n",
    "#2. Convert into orc/parquet format and write to the target by appending the data on a regular interval\n",
    "#3. Read the data from the target and do the schema evolution and get the evolved dataframe created..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56846659-bd78-4f71-be22-e6f5ed57b5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.orc(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/orcout\",mergeSchema=True).show(2)\n",
    "spark.read.parquet(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/parquetout\",mergeSchema=True).show(2)\n",
    "spark.read.format(\"delta\").load(\"/Volumes/ag_catalog/ag_practice/ag_volume/target/deltaout\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d1d6b33-8169-4811-8acd-c94d1e463da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####5. Reading delta/hive table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbb1fe1-0ac7-4499-b256-a0a43c48d9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"ag_catalog.ag_practice.lh_custtbl\").show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Advanced-Readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
